{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LibVQ \n",
    "Here is a demo to use LibVQ to generate PQ for [Faiss](https://github.com/facebookresearch/faiss).\n",
    "\n",
    "## Install\n",
    "```bash\n",
    "git clone https://github.com/staoxiao/LibVQ.git\n",
    "cd LibVQ\n",
    "pip install .\n",
    "```\n",
    "\n",
    "## Overview\n",
    "There are tow modes to train the PQ:\n",
    "- **LearnableIndex**: train the codebooks with fixed embeddings\n",
    "- **LearnableIndexWithEncoder**: jointly train the codebooks and query/doc encoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare\n",
    "### Data for LearnableIndex\n",
    "\n",
    "If you want to only train the index, you should provide the embeddings and relevance file:\n",
    "\n",
    "- query_embeddings and doc_embeddings matrixs, which are numpy.array or saved in `.npy`.\n",
    "- rel_data and neg_data, whose format is `{query_id: [doc1_id, doc2_id,...]}` (id means the`id`-th row in the embeddings matrix). If not provide neg_data,\n",
    "we will randomly sample negatives form the corpus.\n",
    "\n",
    "More information please refer to [Embeddings and Relevance label](https://github.com/staoxiao/LibVQ/blob/master/LibVQ/dataset/README.md).\n",
    "\n",
    "\n",
    "\n",
    "### Data for LearnableIndexWithEncoder\n",
    "If you want to train the index and encoder jointly, you should provide the raw text data besides the embeddings and relevance file.\n",
    "We take the MSMARCO dataset as an example to show the data preprocess workflow:\n",
    "\n",
    "**1. Download data**\n",
    "\n",
    "For other datasets, you can prepare them following [here](https://github.com/staoxiao/LibVQ/blob/master/LibVQ/dataset/README.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-07-02 14:55:02--  https://rocketqa.bj.bcebos.com/corpus/marco.tar.gz\n",
      "Resolving rocketqa.bj.bcebos.com (rocketqa.bj.bcebos.com)... 103.235.46.61, 2409:8c04:1001:1002:0:ff:b001:368a\n",
      "Connecting to rocketqa.bj.bcebos.com (rocketqa.bj.bcebos.com)|103.235.46.61|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1140510742 (1.1G) [application/x-gzip]\n",
      "Saving to: ‘marco.tar.gz’\n",
      "\n",
      "marco.tar.gz        100%[===================>]   1.06G  6.25MB/s    in 2m 28s  \n",
      "\n",
      "2022-07-02 14:57:32 (7.36 MB/s) - ‘marco.tar.gz’ saved [1140510742/1140510742]\n",
      "\n",
      "--2022-07-02 14:59:09--  https://msmarco.blob.core.windows.net/msmarcoranking/collectionandqueries.tar.gz\n",
      "Resolving msmarco.blob.core.windows.net (msmarco.blob.core.windows.net)... 20.150.34.4\n",
      "Connecting to msmarco.blob.core.windows.net (msmarco.blob.core.windows.net)|20.150.34.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1057717952 (1009M) [application/gzip]\n",
      "Saving to: ‘collectionandqueries.tar.gz’\n",
      "\n",
      "collectionandquerie 100%[===================>]   1009M  19.6MB/s    in 33s     \n",
      "\n",
      "2022-07-02 14:59:44 (30.3 MB/s) - ‘collectionandqueries.tar.gz’ saved [1057717952/1057717952]\n",
      "\n",
      "collection.tsv\n",
      "qrels.dev.small.tsv\n",
      "qrels.train.tsv\n",
      "queries.dev.small.tsv\n",
      "queries.dev.tsv\n",
      "queries.eval.small.tsv\n",
      "queries.eval.tsv\n",
      "queries.train.tsv\n"
     ]
    }
   ],
   "source": [
    "! bash download_data.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess data with your tokenizer\n",
    "text_tokenizer should be a tokenizer class inherits from the PreTrainedTokenizer in huggingface transformers.\n",
    "For example, \n",
    "- BertTokenizer.from_pretrained('bert-uncased-base')\n",
    "- BertTokenizer(your_vocab_file)\n",
    "\n",
    "Noted that the `add_cls_tokens=True` will add the special `[CLS]` token in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_dir = './data/preprocessed_dataset'\n",
    "embedding_dir = './Results/ARG'\n",
    "max_query_length = 32\n",
    "max_doc_length = 256\n",
    "\n",
    "# preprocess\n",
    "from LibVQ.dataset.preprocess import preprocess_data\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "preprocess_data(data_dir='./data/dataset/',\n",
    "                output_dir=preprocess_dir,\n",
    "                text_tokenizer=AutoTokenizer.from_pretrained('Shitao/msmarco_query_encoder'),\n",
    "                add_cls_tokens=True,\n",
    "                max_doc_length=max_doc_length,\n",
    "                max_query_length=max_query_length,\n",
    "                workers_num=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Embeddings with your encoder\n",
    "Mention your own encoder model in a class wihch has token sequence as input and output the sentence embedding.\n",
    "\n",
    "```python\n",
    "class YourCustomDEModel:\n",
    "    def forward(input_ids, attention_mask):\n",
    "        return embeddings\n",
    "```\n",
    "In LibVQ, we implement a simple encoder as TransformerModel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start to generate embeddings for corpus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 864/864 [51:39<00:00,  3.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start to generate embeddings for dev queries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start to generate embeddings for train queries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 79/79 [01:15<00:00,  1.04it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from LibVQ.inference import get_embeddings\n",
    "from LibVQ.models import Encoder, TransformerModel\n",
    "\n",
    "# Load encoder\n",
    "query_encoder = TransformerModel.from_pretrained('Shitao/msmarco_query_encoder')\n",
    "doc_encoder = TransformerModel.from_pretrained('Shitao/msmarco_doc_encoder')\n",
    "text_encoder = Encoder(query_encoder, doc_encoder)\n",
    "emb_size = query_encoder.encoder.config.hidden_size\n",
    "\n",
    "# generate embeddings\n",
    "doc_embeddings, dev_query, train_query = get_embeddings(data_dir=preprocess_dir,\n",
    "               encoder=text_encoder,\n",
    "               max_doc_length=max_doc_length,\n",
    "               max_query_length=max_query_length,\n",
    "               output_dir=embedding_dir,\n",
    "               batch_size=10240)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate data for distillation\n",
    "If there is no labeled data, you can generate the top-k docs for train queries as train data.\n",
    "\n",
    "You can use a flat index or a opq index (more efficiency but lower accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LibVQ.dataset.preprocess import generate_virtual_traindata\n",
    "\n",
    "generate_virtual_traindata(\n",
    "        doc_embeddings,\n",
    "        train_query,\n",
    "        output_dir = embedding_dir,\n",
    "        use_gpu=False,\n",
    "        topk= 400,\n",
    "        index_method = 'opq',\n",
    "        subvector_num=32,\n",
    "        subvector_bits=8,\n",
    "        dist_mode='ip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training of PQ\n",
    "Two steps:\n",
    "- Create a index: LearnableIndex()\n",
    "- Train the index: LearnableIndex.fit_with_multi_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# settings:\n",
    "index_method = 'opq'\n",
    "ivf_centers_num = -1\n",
    "subvector_num = 32\n",
    "subvector_bits = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distill-VQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dev-rels.tsv: 7437it [00:00, 847153.50it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 110/110 [00:31<00:00,  3.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of query:6980,  searching time per query: 0.004520652458114406\n",
      "6980 matching queries found\n",
      "MRR@10:0.35032035975803927\n",
      "MRR@100:0.3615070309827782\n",
      "Recall@10:0.6261103151862463\n",
      "Recall@30:0.7776981852913089\n",
      "Recall@50:0.8307425978987586\n",
      "Recall@100:0.8865926456542501\n"
     ]
    }
   ],
   "source": [
    "import faiss, os\n",
    "from LibVQ.dataset.dataset import write_rel\n",
    "from LibVQ.learnable_index import LearnableIndex\n",
    "\n",
    "faiss.omp_set_num_threads(32)\n",
    "\n",
    "doc_embeddings_file = os.path.join(embedding_dir, 'docs.memmap')\n",
    "query_embeddings_file = os.path.join(embedding_dir, 'train-queries.memmap')\n",
    "save_ckpt_dir = f'./saved_ckpts/distill-VQ/'\n",
    "\n",
    "\n",
    "# Create Index\n",
    "learnable_index = LearnableIndex(index_method=index_method,\n",
    "                                 doc_embeddings=doc_embeddings,\n",
    "                                 ivf_centers_num=ivf_centers_num,\n",
    "                                 subvector_num=subvector_num,\n",
    "                                 subvector_bits=subvector_bits)\n",
    "\n",
    "# distill with generated data\n",
    "learnable_index.fit_with_multi_gpus(rel_file=os.path.join(embedding_dir, 'train-virtual_rel.tsv'),\n",
    "                                    neg_file=os.path.join(embedding_dir,\n",
    "                                                          f\"train-queries-virtual_hardneg.pickle\"),\n",
    "                                    query_embeddings_file=query_embeddings_file,\n",
    "                                    doc_embeddings_file=doc_embeddings_file,\n",
    "                                    emb_size=emb_size,\n",
    "                                    per_query_neg_num=1,\n",
    "                                    checkpoint_path=save_ckpt_dir,\n",
    "                                    logging_steps=100,\n",
    "                                    per_device_train_batch_size=512,\n",
    "                                    loss_weight={'encoder_weight': 0.0, 'pq_weight': 1.0,\n",
    "                                                 'ivf_weight': 0.0},\n",
    "                                    lr_params={'encoder_lr': 0.0, 'pq_lr': 1e-4, 'ivf_lr': 0.0},\n",
    "                                    loss_method='distill',\n",
    "                                    epochs=30)\n",
    "\n",
    "ground_truths = load_rel(os.path.join(preprocess_dir, 'dev-rels.tsv'))\n",
    "learnable_index.test(dev_query, ground_truths, topk=1000, batch_size=64,\n",
    "           MRR_cutoffs=[10, 100], Recall_cutoffs=[10, 30, 50, 100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distill-VQ with Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LibVQ.learnable_index import LearnableIndexWithEncoder\n",
    "\n",
    "save_ckpt_dir = f'./saved_ckpts/distill-VQ-Encoder/'\n",
    "\n",
    "learnable_index_with_encoder = LearnableIndexWithEncoder(index_method=index_method,\n",
    "                                 encoder=text_encoder,\n",
    "                                 doc_embeddings=doc_embeddings,\n",
    "                                 subvector_num=subvector_num,\n",
    "                                 subvector_bits=subvector_bits)\n",
    "\n",
    "\n",
    "learnable_index_with_encoder.fit(rel_data=os.path.join(embedding_dir, 'train-virtual_rel.tsv'),\n",
    "                                neg_data=os.path.join(embedding_dir,\n",
    "                                                      f\"train-queries-virtual_hardneg.pickle\"),\n",
    "                                query_data_dir=preprocess_dir,\n",
    "                                max_query_length=max_query_length,\n",
    "                                query_embeddings=query_embeddings_file,\n",
    "                                doc_embeddings=doc_embeddings_file,\n",
    "                                emb_size=emb_size,\n",
    "                                per_query_neg_num=1,\n",
    "                                checkpoint_path=save_ckpt_dir,\n",
    "                                logging_steps=100,\n",
    "                                per_device_train_batch_size=512,\n",
    "                                loss_weight={'encoder_weight': 1.0, 'pq_weight': 1.0,\n",
    "                                             'ivf_weight': 0.0},\n",
    "                                lr_params={'encoder_lr': 1e-5, 'pq_lr': 1e-4, 'ivf_lr': 0.0},\n",
    "                                loss_method='distill',\n",
    "                                epochs=10)\n",
    "\n",
    "# get new query embeddigns because the encoder is updated in the training\n",
    "new_query_embeddings = learnable_index_with_encoder.encode(data_dir=preprocess_dir,\n",
    "                                              prefix='dev-queries',\n",
    "                                              max_length=max_query_length,\n",
    "                                              output_dir='LearnableIndexWithEncoder',\n",
    "                                              batch_size=8196,\n",
    "                                              is_query=True,\n",
    "                                              return_vecs=True\n",
    "                                              )\n",
    "    \n",
    "\n",
    "ground_truths = load_rel(os.path.join(preprocess_dir, 'dev-rels.tsv'))\n",
    "learnable_index_with_encoder.test(dev_query, ground_truths, topk=1000, batch_size=64,\n",
    "           MRR_cutoffs=[10, 100], Recall_cutoffs=[10, 30, 50, 100])"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "interpreter": {
   "hash": "955b5dbf1c9568fcdca6427b82ed9dd3b5652756bab72d537c2112a7dd2607f3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython2",
  "version": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}